#!/bin/bash
set -e

##############################################################################################
# # This script will setup Spack and best practices for a few applications.                  #
# # Use as postinstall in AWS ParallelCluster (https://docs.aws.amazon.com/parallelcluster/) #
##############################################################################################

print_help() {
    cat <<EOF
Usage: postinstall.sh [-fg] [-v] [spec] [spec...]

Installs spack on a parallel cluster with recommended configurations.
Spack directory is cloned into a shared directory and recommended packages are
configured.

As the install can take more than an hour (especially on smaller instances), the
default is to run the script in the background so that it can be used as
pcluster's postinstall CloudFormation script without timing out during cluster
initialization.  Output can be monitored in /var/log/spack-postinstall.log

Options:
 -h|--help              Print this help message.
 -fg                    Run in foreground, without logs, blocking until
                        complete.
                        Ensure ssh connection is stable.
 -v                     Be verbose during execution
 --[no-]buildcache      Use the generic target buildcache provided by the
                        aws-pcluster-[x86_64_v4,neoverse_v1] stacks. Enabled by
                        default for Amazon Linux2, disabled for other OSs.
 --no-arm-compiler      Don't install ARM compilers (ACFL)
 --no-intel-compiler    Don't install Intel compilers
 --prefix <path>        This script will install Spack into the first shared drive
                        found on ParallelCluster or $HOME if no shared drive can be
                        found. Override the location with <path>.
 [spec]                 "spec" to be installed after initial
                        configuration: e.g., gcc@12.3.0 or "gcc @ 13.0".
                        Multiple specs can be listed, but they need to either
                        be quoted or contain no spaces.

Developer Options:
 --config-branch        Pull configuration (e.g., packages.yaml) from this
                        branch.  Default is "main".
 --config-repo          Pull configuration (e.g., packages.yaml) from this
                        github user/repo.  Default is "spack/spack-configs".
 --spack-branch         Clone spack using this branch.  Default is "develop"
 --spack-repo           Clone spack using this github user/repo. Default is
                        "spack/spack".
EOF
}

# CONFIG_REPO: a the user/repo on github to use for configuration.a custom user/repo/branch in case users wish to
# provide or test their own configurations.
export CONFIG_REPO="spack/spack-configs"
export CONFIG_BRANCH="main"
export SPACK_REPO="spack/spack"
export SPACK_BRANCH="develop"
export PREFIX=""
install_specs=()
export generic_buildcache=""
export EXTERNAL_SPACK="false"
install_in_foreground=false
while [ $# -gt 0 ]; do
    case $1 in
        --buildcache )
            generic_buildcache=true
            shift
            ;;
        --config-branch )
            CONFIG_BRANCH="$2"
            shift 2
            ;;
        --config-repo )
            CONFIG_REPO="$2"
            shift 2
            ;;
        -fg )
            install_in_foreground=true
            shift
            ;;
        -h|--help )
            print_help
            exit 0
            ;;
        --no-arm-compiler )
            export NO_ARM_COMPILER=1
            shift
            ;;
        --no-buildcache )
            generic_buildcache=false
            shift
            ;;
        --no-intel-compiler )
            export NO_INTEL_COMPILER=1
            shift
            ;;
        --prefix )
            PREFIX="$2"
            shift 2
            ;;
        --spack-branch )
            SPACK_BRANCH="$2"
            shift 2
            ;;
        --spack-repo )
            SPACK_REPO="$2"
            shift 2
            ;;
        -v )
            set -v
            shift
            ;;
        -* )
            print_help
            echo
            echo "ERROR: Unknown option: $1"
            exit 1
            ;;
        * )
            echo "Going to install: $1"
            install_specs+=("${1}")
            shift
            ;;
    esac
done

setup_variables() {
    # Determine default user
    [ -f /etc/os-release ] && . /etc/os-release
    case "$ID" in
        amzn|rhel)
            default_user="ec2-user"
            ;;
        centos|ubuntu|rocky)
            default_user="${ID}"
            ;;
        *)
            default_user=""
            ;;
    esac

    # Install onto first shared storage device
    cluster_config="/opt/parallelcluster/shared/cluster-config.yaml"
    pip3 install pyyaml
    if [ -f "${cluster_config}" ]; then
        cfn_ebs_shared_dirs=$(python3 << EOF
#/usr/bin/env python
import yaml
with open("${cluster_config}", 'r') as s:
    print(yaml.safe_load(s)["SharedStorage"][0]["MountDir"])
EOF
                           )
        scheduler=$(python3 << EOF
#/usr/bin/env python
import yaml
with open("${cluster_config}", 'r') as s:
    print(yaml.safe_load(s)["Scheduling"]["Scheduler"])
EOF
                 )
    elif [ -f /etc/parallelcluster/cfnconfig ]; then
        . /etc/parallelcluster/cfnconfig
    else
        echo "Cannot find ParallelCluster configs"
        cfn_ebs_shared_dirs=""
    fi

    # If no shared drives are configured, try finding a Lustre, then an EFS
    [ -z "${cfn_ebs_shared_dirs}" ] && cfn_ebs_shared_dirs="$(mount -t lustre | cut -d\  -f 3 | head -n1)"
    [ -z "${cfn_ebs_shared_dirs}" ] && cfn_ebs_shared_dirs="$(mount -t efs | cut -d\  -f 3 | head -n1)"

    # If we cannot find any shared directory, use default_user's $HOME
    [ -z "${cfn_ebs_shared_dirs}" ] && cfn_ebs_shared_dirs=$(awk -F: '/'"${default_user}"'/{print $6}' /etc/passwd)

    # Override install location with PREFIX if set
    cfn_ebs_shared_dirs="${PREFIX:-${cfn_ebs_shared_dirs}}"

    # Use external Spack if $SPACK_ROOT is already set
    install_path=${SPACK_ROOT:-"${cfn_ebs_shared_dirs}/spack"}
    tmp_path="$(mktemp -d -p "${cfn_ebs_shared_dirs}")" && export TMPDIR="${tmp_path}"

    echo "Installing Spack into ${install_path}."

    if [ "true" == "${generic_buildcache}" ] && [ "Amazon Linux 2" != "${PRETTY_NAME}" ]; then
        echo "Generic buildcache only available for Alinux2."
    fi
    if [ -z "${generic_buildcache}" ] && [ "Amazon Linux 2" == "${PRETTY_NAME}" ]; then
        generic_buildcache=true
    else
        generic_buildcache=false
    fi
}

major_version() {
    pcluster_version=$(grep -oE '[0-9]*\.[0-9]*\.[0-9]*' /opt/parallelcluster/.bootstrapped)
    echo "${pcluster_version/\.*}"
}

# Make first user owner of Spack installation when script exits.
cleanup() {
    rc=$?
    if ! ${EXTERNAL_SPACK}
    then
        chown -R ${default_user}:${default_user} "${install_path}"
    fi
    [ -d "${tmp_path}" ] && rm -rf "${tmp_path}"
    exit $rc
}

download_spack() {
    if [ -z "${SPACK_ROOT}" ]
    then
        if [ -n "${SPACK_BRANCH}" ]
        then
            git clone -c feature.manyFiles=true "https://github.com/${SPACK_REPO}" -b "${SPACK_BRANCH}" "${install_path}"
        elif [ -n "${spack_commit}" ]
        then
            git clone -c feature.manyFiles=true "https://github.com/${SPACK_REPO}" "${install_path}"
            cd "${install_path}" && git checkout "${spack_commit}"
        fi
    fi
}

# zen3 EC2 instances (e.g. hpc6a) is misidentified as zen2 so zen3 packages are found under packages-zen2.yaml.
target() {
    (
        . "${install_path}/share/spack/setup-env.sh"
        spack arch -t
    )
}

stack_arch() {
    (
        . "${install_path}/share/spack/setup-env.sh"
        case $(arch) in
            aarch64)
                # neoverse_n1 packages also included in neoverse_v1 stack
                spack arch -t | sed -e 's?neoverse_n1?neoverse_v1?g'
                ;;
            x86_64*)
                # x86_64_v3 packages also included in x86_64_v4 stack
                spack arch -g | sed -e 's?x86_64_v3?x86_64_v4?g'
                ;;
        esac
    )
}

download_packages_yaml() {
    # $1: spack target
    . "${install_path}/share/spack/setup-env.sh"
    target="${1}"
    curl -Ls "https://raw.githubusercontent.com/${CONFIG_REPO}/${CONFIG_BRANCH}/AWS/parallelcluster/packages-${target}.yaml" -o /tmp/packages.yaml
    if [ "$(cat /tmp/packages.yaml)" = "404: Not Found" ]; then
        # Pick up parent if current generation is not available
        for target in $(spack-python -c 'print(" ".join(spack.platforms.host().target("'"${target}"'").to_dict()["parents"]))'); do
            if [ -z "${target}" ] ; then
                echo "Cannot find suitable packages.yaml"
                exit 1
            fi
            download_packages_yaml "${target}"
            # Exit loop if found here or in deeper level.
            [ "$(cat /tmp/packages.yaml)" = "404: Not Found" ] || break &>/dev/null
        done
    fi
}

set_modules() {
    mkdir -p "${install_path}/etc/spack"
    curl -Ls "https://raw.githubusercontent.com/${CONFIG_REPO}/${CONFIG_BRANCH}/AWS/parallelcluster/modules.yaml" \
         -o "${install_path}/etc/spack/modules.yaml"
}

set_variables() {
    # Set versions of pre-installed software in packages.yaml
    [ -z "${SLURM_ROOT}" ] && ls /etc/systemd/system/slurm* &>/dev/null && \
        SLURM_ROOT=$(dirname $(dirname "$(awk '/ExecStart=/ {print $1}' /etc/systemd/system/slurm* | sed -e 's?^.*=??1' | head -n1)"))
    # Fallback to default location if SLURM not in systemd
    [ -z "${SLURM_ROOT}" ] && [ -d "/opt/slurm" ] && SLURM_ROOT=/opt/slurm
    [ -z "${SLURM_VERSION}" ] && SLURM_VERSION=$(strings "${SLURM_ROOT}"/lib/libslurm.so | grep -e '^VERSION' | awk '{print $2}' | sed -e 's?"??g')
    [ -z "${LIBFABRIC_VERSION}" ] && LIBFABRIC_VERSION=$(awk '/Version:/{print $2}' "$(find /opt/amazon/efa/ -name libfabric.pc | head -n1)" | sed -e 's?~??g' -e 's?amzn.*??g')
    export SLURM_VERSION SLURM_ROOT LIBFABRIC_VERSION

    # Turn off generic_buildcache if CI stack does not exist
    [ -f "${SPACK_ROOT}/share/spack/gitlab/cloud_pipelines/stacks/aws-pcluster-$(stack_arch)/packages.yaml" ] || generic_buildcache=false
}

set_pcluster_defaults() {
    # Write the above as actual yaml file and only parse the \$.
    mkdir -p "${install_path}/etc/spack"
    ( download_packages_yaml "$(target)" )

    if [ "$(cat /tmp/packages.yaml)" != "404: Not Found" ]; then
        envsubst < /tmp/packages.yaml > "${install_path}/etc/spack/packages.yaml"
    fi
}

load_spack_at_login() {
    if [ -z "${SPACK_ROOT}" ]
    then
        case "${scheduler}" in
            slurm)
                echo -e "\n# Spack setup from Github repo spack-configs" >> /opt/slurm/etc/slurm.sh
                echo -e "\n# Spack setup from Github repo spack-configs" >> /opt/slurm/etc/slurm.csh
                echo ". ${install_path}/share/spack/setup-env.sh &>/dev/null || true" >> /opt/slurm/etc/slurm.sh
                echo ". ${install_path}/share/spack/setup-env.csh &>/dev/null || true" >> /opt/slurm/etc/slurm.csh
                ;;
            *)
                echo "WARNING: Spack will need to be loaded manually when ssh-ing to compute instances."
                echo ". ${install_path}/share/spack/setup-env.sh" > /etc/profile.d/spack.sh
                echo ". ${install_path}/share/spack/setup-env.csh" > /etc/profile.d/spack.csh
        esac
    fi
}
setup_bootstrap_mirrors() {
    . "${install_path}/share/spack/setup-env.sh"

    # `gcc@12.3.0` is created as part of building the containers in https://github.com/spack/gitlab-runners
    # and mirrored onto `$bootstrap_gcc_cache`:
    bootstrap_gcc_cache="https://bootstrap.spack.io/pcluster/$(spack arch -o)/$(arch)"
    if curl -fIsLo /dev/null "${bootstrap_gcc_cache}/build_cache/index.json"; then
        spack mirror add --scope=site bootstrap-gcc-cache "${bootstrap_gcc_cache}"
    fi

    # Newer gpg2 versions on Ainux2 will not be able to validate the key. This allows spack to accept the buildcache keys.
    if [ "$(gpg --version | awk '/gpg/{print $3}')" == "2.0.22" ]; then
        mkdir -m 700 -p ${SPACK_ROOT}/opt/spack/gpg
        echo "openpgp" >> ${SPACK_ROOT}/opt/spack/gpg/gpg.conf
    fi
    # Add keys if binary mirrors have been added
    spack-python -c 'sys.exit(len(spack.mirror.MirrorCollection(binary=True)._mirrors.values()) == 0)' && spack buildcache keys -it
}

setup_pcluster_buildcache_stack() {
    . "${install_path}/share/spack/setup-env.sh"
    # Make sure the subshell which installs the Intel compiler finds the correct installation prefix
    echo -e "config:\n  install_tree:\n    root: ${SPACK_ROOT}/opt/spack\n" > $SPACK_ROOT/etc/spack/config.yaml
    export SPACK_CI_STACK_NAME="aws-pcluster-$(stack_arch)"
    bash "${SPACK_ROOT}/share/spack/gitlab/cloud_pipelines/scripts/pcluster/setup-pcluster.sh"
    rm -f $SPACK_ROOT/etc/spack/config.yaml
    # Fix SLURM location if it's not the ParallelCluster standard /opt/slurm
    [ -d "${SLURM_ROOT}" ] && sed -i -e "s?/opt/slurm?${SLURM_ROOT}?g" "${SPACK_ROOT}"/etc/spack/packages.yaml
}

setup_spack() {
    . "${install_path}/share/spack/setup-env.sh"
    spack compiler add --scope site
    # Do not add  autotools/buildtools packages. These versions need to be managed by spack or it will
    # eventually end up in a version mismatch (e.g. when compiling gmp).
    spack external find --scope site --tag core-packages

    # Remove system installed `gcc` if there is no matching `g++`. Missing `gfortran` is OK,
    # since gcc & g++ are sufficient for `spack install gcc@gcc`.
    for compiler in $(spack compiler list | grep -v '-' |grep -v '=>' | xargs); do
        spack compiler info --scope=site ${compiler} 2>/dev/null | grep -E 'cc =|cxx =' | grep -q " None" && spack compiler rm --scope=site ${compiler}
    done
}

patch_compilers_yaml() {
    # Graceful exit if package not found by spack
    set -o pipefail
    compilers_yaml="${SPACK_ROOT}/etc/spack/compilers.yaml"
    [ -f "${compilers_yaml}" ] || {
        echo "Cannot find ${compilers_yaml}, compiler setup might now be optimal."
        return
    }

    # System ld is too old for amzn linux2
    spack_gcc_version=$(spack find --format '{version}' gcc | xargs -n1 | tail -n1)
    binutils_path=$(spack find -p binutils | awk '/binutils/ {print $2}' | head -n1)
    if [ -d "${binutils_path}" ] && [ -n "${spack_gcc_version}" ]; then python3 <<EOF
import yaml

with open("${compilers_yaml}",'r') as f:
    compilers=yaml.safe_load(f)

for c in compilers["compilers"]:
    if "aocc" in c["compiler"]["spec"] or "arm" in c["compiler"]["spec"] or "intel" in c["compiler"]["spec"] or "oneapi" in c["compiler"]["spec"] \
       or "${spack_gcc_version}" in c["compiler"]["spec"]:
           if "prepend_path" in compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["environment"]:
               compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["environment"]["prepend_path"]["PATH"] = "${binutils_path}/bin"
           else:
               compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["environment"] = {"prepend_path":{"PATH":"${binutils_path}/bin"}}

with open("${compilers_yaml}",'w') as f:
    yaml.dump(compilers, f)
EOF
    fi

    # Oneapi needs extra_rpath to gcc libstdc++.so.6
    if oneapi_gcc_version=$(spack find --format '{compiler}' intel-oneapi-compilers 2>/dev/null | sed -e 's/=//g') && \
            [ -n "${oneapi_gcc_version}" ] && oneapi_gcc_path=$(spack find -p "${oneapi_gcc_version}" | grep "${oneapi_gcc_version}" | awk '{print $2}' | head -n1) && \
            [ -d "${oneapi_gcc_path}" ]; then  python3 <<EOF
import yaml

with open("${compilers_yaml}",'r') as f:
    compilers=yaml.safe_load(f)

for c in compilers["compilers"]:
    if "oneapi" in c["compiler"]["spec"]:
        compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["extra_rpaths"] = ["${oneapi_gcc_path}/lib64"]

with open("${compilers_yaml}",'w') as f:
    yaml.dump(compilers, f)
EOF
    fi

    # TODO: Who needs this? WRF? gromacs will not build when this is active.
    # Armclang needs to find its own libraries
    if acfl_path=$(spack find -p acfl 2>/dev/null | awk '/acfl/ {print $2}') && \
            [ -d "${acfl_path}" ] && cpp_include_path=$(dirname "$(find "${acfl_path}" -name cassert)") && \
            [ -d "${cpp_include_path}" ]; then python3 <<EOF
import yaml

with open("${compilers_yaml}",'r') as f:
    compilers=yaml.safe_load(f)

for c in compilers["compilers"]:
    if "arm" in c["compiler"]["spec"]:
        if "prepend_path" in compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["environment"]:
            compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["environment"]["prepend_path"]["CPATH"] = "${cpp_include_path}:${cpp_include_path}/aarch64-linux-gnu"
        else:
            compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["environment"] = {"prepend_path":{"CPATH":"${cpp_include_path}:${cpp_include_path}/aarch64-linux-gnu"}}

with open("${compilers_yaml}",'w') as f:
    yaml.dump(compilers, f)
EOF
    fi

    # Armclang needs extra_rpath to libstdc++.so
    # TODO: FYI ACFL installer does this by adding libstdc++.so.6 path to LD_LIBRARY_PATH
    if acfl_path=$(spack find -p acfl 2>/dev/null | awk '/acfl/ {print $2}') && \
            acfl_libstdcpp_path=$(dirname "$(find "${acfl_path}" -name libstdc++.so | head -n1)") && \
            [ -d "${acfl_libstdcpp_path}" ]; then python3 <<EOF
import yaml

with open("${compilers_yaml}",'r') as f:
    compilers=yaml.safe_load(f)

for c in compilers["compilers"]:
    if "arm" in c["compiler"]["spec"]:
        compilers["compilers"][compilers["compilers"].index(c)]["compiler"]["extra_rpaths"] = ["${acfl_libstdcpp_path}"]

with open("${compilers_yaml}",'w') as f:
    yaml.dump(compilers, f)
EOF
    fi
}

install_compilers() {
    if [ -z "${SPACK_ROOT}" ]; then
        if [ -f /opt/slurm/etc/slurm.sh ]; then
            . /opt/slurm/etc/slurm.sh
        else
            . /etc/profile.d/spack.sh
        fi
    fi

    # Compiler needed for all kinds of codes. It makes no sense not to install it.
    mirrors_entry=$(grep -s bootstrap-gcc-cache $SPACK_ROOT/etc/spack/mirrors.yaml)
    if [ -n "${mirrors_entry}" ]; then
        # Make sure we select the gcc from the cache we just added (temporarily override all mirrors.yaml configuration files):
        tmpdir=$(mktemp -d)
        echo -e "mirrors::\n${mirrors_entry}" > "${tmpdir}/mirrors.yaml"
        gcc_hash=$(spack --config-scope ${tmpdir} buildcache list -a -v -l gcc | grep -v -- "----" | tail -n1 | awk '{print $1}')
    fi

    # Try to install from bootstrap-gcc-buildcache, fall back to generic version.
    spack install /${gcc_hash} 2>/dev/null || spack install gcc
    (
        spack load gcc
        spack compiler add --scope site
    )

    if [ -z "${NO_INTEL_COMPILER}" ] && [ "x86_64" == "$(arch)" ]
    then
        # Add oneapi@latest & intel@latest
        spack install intel-oneapi-compilers-classic
        bash -c ". "$(spack location -i intel-oneapi-compilers)/setvars.sh"; spack compiler add --scope site"
    fi
}

# TODO: Handle this compiler in buildcache stack once wrf%acfl package gets added
install_acfl() {
    if [ -z "${SPACK_ROOT}" ]; then
        if [ -f /opt/slurm/etc/slurm.sh ]; then
            . /opt/slurm/etc/slurm.sh
        else
            . /etc/profile.d/spack.sh
        fi
    fi

    if [ -z "${NO_ARM_COMPILER}" ] && [ "aarch64" == "$(arch)" ]
    then
        spack install acfl
        (
            spack load acfl
            spack compiler add --scope site
        )
    fi
}

setup_mirrors() {
    . "${install_path}/share/spack/setup-env.sh"

    if ${generic_buildcache}; then
        spack mirror add --scope site "aws-pcluster-$(stack_arch)" "https://binaries.spack.io/develop/aws-pcluster-$(stack_arch)"
    fi
    # Add older specific target mirrors if they exist
    mirror_url="https://binaries.spack.io/develop/aws-pcluster-$(target | sed -e 's?_avx512??1')"
    if curl -fIsLo /dev/null "${mirror_url}/build_cache/index.json"; then
        spack mirror add --scope site "aws-pcluster-legacy" "${mirror_url}"
    fi
    spack-python -c 'sys.exit(len(spack.mirror.MirrorCollection(binary=True)._mirrors.values()) == 0)' && spack buildcache keys -it
}

install_packages() {
    # Install any specs provided to the script.
    for spec in "$@"
    do
        [ -z "${spec}" ] || spack install -U "${spec}"
    done
}

if [ -f "/opt/parallelcluster/.bootstrapped" ] && [ "3" != "$(major_version)" ]; then
    echo "ParallelCluster version $(major_version) not supported."
    exit 1
fi

[ -n "${SPACK_ROOT}" ] && EXTERNAL_SPACK="true"

tmpfile=$(mktemp)
echo "$(declare -pf)
    trap \"cleanup\" SIGINT EXIT
    setup_variables
    [ -d \"${install_path}\" ] && EXTERNAL_SPACK=\"true\"
    download_spack
    set_modules
    load_spack_at_login
    setup_bootstrap_mirrors
    set_variables
    if \${generic_buildcache}; then
       setup_pcluster_buildcache_stack
    else
        set_pcluster_defaults
        setup_spack
        install_compilers
        install_acfl
        patch_compilers_yaml
    fi
    setup_mirrors
    install_packages \"${install_specs}\"
    echo \"*** Spack setup completed ***\"
    rm -f ${tmpfile}
" > ${tmpfile}

if ${install_in_foreground}; then
    bash ${tmpfile}
else
    nohup bash ${tmpfile} &> /var/log/spack-postinstall.log &
    disown -a
fi
